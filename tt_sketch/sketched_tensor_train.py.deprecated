"""Implements the small-sketch algorithm for sparse tensors"""
from __future__ import annotations

from functools import cached_property
from re import L
from typing import Callable, Dict, List, Optional, Sequence, Tuple, Type

import numpy as np
import numpy.typing as npt

from tt_sketch.drm import (
    DenseGaussianDRM,
    SparseGaussianDRM,
    TensorTrainDRM,
    ALL_DRM,
)
from tt_sketch.drm_base import DRM, CanSlice, CanIncreaseRank
from tt_sketch.sketching_methods import (
    SKETCHING_METHODS,
    TENSOR_SKETCH_DISPATCH,
    CansketchDense,
    CansketchSparse,
    CansketchTT,
    CansketchCP,
)
from tt_sketch.tensor import Tensor, TensorTrain, TensorSum
from tt_sketch.utils import ArrayList, TTRank, process_tt_rank, right_mul_pinv
from tt_sketch.sketch_container import SketchContainer

DEFAULT_DRM = {
    CansketchDense: DenseGaussianDRM,
    CansketchSparse: SparseGaussianDRM,
    CansketchTT: TensorTrainDRM,
    CansketchCP: TensorTrainDRM,
}

BlockedSketch = Dict[Tuple[int, int], SketchContainer]


SKETCH_METHOD_SIGNATURE = Callable[[Tensor, DRM, DRM], SketchContainer]


class SketchedTensorTrain(Tensor):
    left_drm: DRM
    right_drm: DRM
    sketch_: SketchContainer

    @property
    def Psi_cores(self) -> ArrayList:
        return self.sketch_.Psi_cores

    @property
    def Omega_mats(self) -> ArrayList:
        return self.sketch_.Omega_mats

    def __init__(
        self,
        left_drm: DRM,
        right_drm: DRM,
        sketch: Optional[SketchContainer] = None,
    ):
        self.left_drm = left_drm
        self.right_drm = right_drm
        if sketch is None:
            left_rank = left_drm.rank
            right_rank = right_drm.rank[::-1]
            shape = left_drm.shape
            sketch = SketchContainer.zero(shape, left_rank, right_rank)
        self.sketch_ = sketch
        self.shape = left_drm

    def add(self, other: Tensor):
        sketch_other = self.sketch(other)

        sketch_new = self.sketch_ + sketch_other
        return self.__class__(self.left_drm, self.right_drm, sketch_new)

    __add__ = add
    __radd__ = add

    def sketch(self, tensor: Tensor) -> SketchContainer:
        for tensor_type, sketch_type in TENSOR_SKETCH_DISPATCH.items():
            if isinstance(tensor, tensor_type):
                sketch_method = SKETCHING_METHODS[sketch_type]
        sketch = sketch_method(  # type: ignore
            tensor, self.left_drm, self.right_drm
        )
        return sketch

    def increase_rank(
        self,
        tensor: Tensor,
        new_left_rank: Tuple[int, ...],
        new_right_rank: Tuple[int, ...],
    ) -> SketchedTensorTrain:
        for drm in (self.left_drm, self.right_drm):
            if not isinstance(drm, CanSlice):
                drm_name = drm.__class__.__name__
                raise ValueError(
                    f"Increasing rank is not supported for DRM {drm_name}"
                )

        n_dims = len(tensor.shape)
        left_rank_slices = [
            (0,) * (n_dims - 1),
            self.left_drm.rank,
            new_left_rank,
        ]
        right_rank_slices = [
            (0,) * (n_dims - 1),
            self.right_drm.rank[::-1],
            new_right_rank,
        ]
        left_sketch = self.left_drm.increase_rank(new_left_rank)  # type: ignore
        right_sketch = self.right_drm.increase_rank(  # type: ignore
            new_right_rank
        )

        sketch_dict = _blocked_sketch_components(
            tensor,
            left_sketch,
            right_sketch,
            left_rank_slices,
            right_rank_slices,
            excluded_entries=[(0, 0)],
        )

        sketch_dict[(0, 0)] = self.sketch_
        sketch = _assemble_blocked_sketches(
            left_rank_slices, right_rank_slices, tensor.shape, sketch_dict
        )

        return self.__class__(left_sketch, right_sketch, sketch)

    @classmethod
    def zero(cls, left_drm: DRM, right_drm: DRM) -> SketchedTensorTrain:
        """Initialize a new ``SketchedTensorTrain`` with zeros as ``Psi_cores``
        ``Omega_mats``."""
        left_rank = left_drm.rank
        right_rank = right_drm.rank[::-1]
        shape = left_drm.shape
        sketch = SketchContainer.zero(shape, left_rank, right_rank)
        return cls(left_drm, right_drm, sketch)

    @classmethod
    def from_tensor(
        cls,
        tensor: Tensor,
        left_rank: TTRank,
        right_rank: TTRank,
        seed: Optional[int] = None,
        left_sketch_type: Optional[Type[DRM]] = None,
        right_sketch_type: Optional[Type[DRM]] = None,
    ) -> SketchedTensorTrain:
        """Initialize from a tensor

        Infers the correct sketching algorithm based on the type of ``tensor``.
        """
        sketch_method, default_drm_type = get_sketch_method_default_drm(tensor)

        if left_sketch_type is None:
            if right_sketch_type is not None:
                left_sketch_type = right_sketch_type
            else:
                left_sketch_type = default_drm_type
        if right_sketch_type is None:
            if left_sketch_type is not None:
                right_sketch_type = left_sketch_type
            else:
                right_sketch_type = default_drm_type
        d = len(tensor.shape)
        if seed is None:
            seed = np.mod(hash(np.random.uniform()), 2**32)
        left_rank = process_tt_rank(left_rank, tensor.shape, trim=False)
        right_rank = process_tt_rank(right_rank, tensor.shape, trim=False)
        left_sketch = left_sketch_type(
            left_rank, transpose=False, shape=tensor.shape, seed=seed
        )
        right_seed = np.mod(seed + hash(str(d)), 2**32)
        right_sketch = right_sketch_type(
            right_rank, transpose=True, shape=tensor.shape, seed=right_seed
        )

        sketch = sketch_method(tensor, left_sketch, right_sketch)

        sketched = cls(left_sketch, right_sketch, sketch)
        return sketched

    @cached_property
    def C_cores(self) -> ArrayList:
        return tt_cores_from_sketch(self.sketch_)

    @property
    def T(self) -> SketchedTensorTrain:
        new_sketch = self.sketch_.T
        return self.__class__(self.right_drm, self.left_drm, new_sketch)

    def to_tt(self) -> TensorTrain:
        return TensorTrain(self.C_cores)

    def to_numpy(self) -> npt.NDArray[np.float64]:
        return self.to_tt().to_numpy()


def get_sketch_method_default_drm(
    tensor: Tensor,
) -> Tuple[SKETCH_METHOD_SIGNATURE, Type[DRM]]:
    """Figure out which sketching method makes the most sense for a tensor"""
    if isinstance(tensor, TensorSum):
        sketch_method = sum_sketch
        matching_drms = set()
        for X in tensor.tensors:
            _, drm = get_sketch_method_default_drm(X)
            matching_drms.add(drm)
        if len(matching_drms) > 0:
            return sketch_method, list(matching_drms)[0]  # type: ignore
    else:
        for tensor_type, sketch_type in TENSOR_SKETCH_DISPATCH.items():
            if isinstance(tensor, tensor_type):
                sketch_method = SKETCHING_METHODS[sketch_type]  # type: ignore
                default_drm_type = DEFAULT_DRM[sketch_type]
                return sketch_method, default_drm_type  # type: ignore

    # No matching method found
    raise ValueError(
        f"""No sketching methods available for tensor of type
        {type(tensor)}"""
    )


def _blocked_sketch_components(
    X_tensor: Tensor,
    left_sketch: CanSlice,
    right_sketch: CanSlice,
    left_rank_slices: List[Tuple[int, ...]],
    right_rank_slices: List[Tuple[int, ...]],
    excluded_entries: Optional[Sequence[Tuple[int, int]]] = None,
) -> BlockedSketch:
    if excluded_entries is None:
        excluded_entries = []
    block_left_sketches = [
        left_sketch.slice(rank1, rank2)
        for rank1, rank2 in zip(left_rank_slices[:-1], left_rank_slices[1:])
    ]
    block_right_sketches = [
        right_sketch.slice(rank1, rank2)
        for rank1, rank2 in zip(right_rank_slices[:-1], right_rank_slices[1:])
    ]

    # Compute all the sketches
    sketch_dict = {}
    for i, left_sketch_slice in enumerate(block_left_sketches):
        for j, right_sketch_slice in enumerate(block_right_sketches):
            if (i, j) in excluded_entries:
                continue

            tt_sketch_block = SketchedTensorTrain.zero(
                left_sketch_slice, right_sketch_slice
            )
            sketch_block = tt_sketch_block.sketch(X_tensor)

            sketch_dict[(i, j)] = sketch_block

    return sketch_dict


def _assemble_blocked_sketches(
    left_rank_slices: List[Tuple[int, ...]],
    right_rank_slices: List[Tuple[int, ...]],
    shape: Tuple[int, ...],
    sketch_dict: BlockedSketch,
) -> SketchContainer:
    left_rank = tuple(left_rank_slices[-1])
    right_rank = tuple(right_rank_slices[-1])

    sketch = SketchContainer.zero(shape, left_rank, right_rank)
    for (i, j), sketch_block in sketch_dict.items():
        left_rank1 = (0,) + left_rank_slices[i]
        left_rank2 = (1,) + left_rank_slices[i + 1]
        right_rank1 = right_rank_slices[j] + (0,)
        right_rank2 = right_rank_slices[j + 1] + (1,)
        for mu, Psi in enumerate(sketch_block.Psi_cores):
            sketch.Psi_cores[mu][
                left_rank1[mu] : left_rank2[mu],
                :,
                right_rank1[mu] : right_rank2[mu],
            ] = Psi
        for mu, Omega in enumerate(sketch_block.Omega_mats):
            sketch.Omega_mats[mu][
                left_rank1[mu + 1] : left_rank2[mu + 1],
                right_rank1[mu] : right_rank2[mu],
            ] = Omega

    return sketch



def get_drm_capabilities():
    """List what all the DRMs are capable of"""
    all_capabilities = {}
    for drm in ALL_DRM:
        drm_capabilities = {}
        for capability in (
            CanSlice,
            CanIncreaseRank,
            CansketchSparse,
            CansketchDense,
            CansketchTT,
        ):
            drm_capabilities[capability.__name__] = issubclass(drm, capability)
        all_capabilities[drm.__name__] = drm_capabilities
    return all_capabilities


def blocked_sketch(
    tensor: Tensor,
    left_drm: CanSlice,
    right_drm: CanSlice,
    left_rank_slices: List[Tuple[int, ...]],
    right_rank_slices: List[Tuple[int, ...]],
) -> SketchContainer:
    """Do a blocked sketch.

    Parameters
    ----------
    tensor
    left_drm
    right_drm
    left_rank_slices
        Instructions how to slice up the left-sketch. If the t--rank of the
        left-sketch is for example (6,8,10), then we can set ``left_rank_slices
        = [(0,0,0), (3,4,5), (6,8,10)]`` to split the left-sketch into two
        equally sized blocks. The number of slices is unlimited.
    right_rank_slices
        Same as ``left_rank_slices`` but pertaining to the right-sketch.
    """
    for drm in (left_drm, right_drm):
        if not isinstance(drm, CanSlice):
            drm_name = drm.__class__.__name__
            raise ValueError(f"Blocked sketch not supported for DRM {drm_name}")

    sketch_dict = _blocked_sketch_components(
        tensor,
        left_drm,
        right_drm,
        left_rank_slices,
        right_rank_slices,
    )

    sketch = _assemble_blocked_sketches(
        left_rank_slices,
        right_rank_slices,
        tensor.shape,
        sketch_dict,
    )

    return sketch


# def _process_summand(
#     summand: Tensor,
#     left_drm: DRM,
#     right_drm: DRM,
# ):
#     sketch_method, _ = get_sketch_method_default_drm(summand)
#     yz = sketch_method(summand, deepcopy(left_drm), deepcopy(right_drm))
#     return yz


# TODO: Put this in sketching_methods
def sum_sketch(
    tensor: TensorSum,
    left_drm: DRM,
    right_drm: DRM,
) -> SketchContainer:
    """Sketch a tensor sum"""
    left_rank = left_drm.rank
    right_rank = right_drm.rank[::-1]
    shape = left_drm.shape
    sketch = SketchContainer.zero(shape, left_rank, right_rank)

    for summand in tensor.tensors:
        sketch_method, _ = get_sketch_method_default_drm(summand)
        sketch_summand = sketch_method(summand, left_drm, right_drm)
        sketch += sketch_summand

    return sketch
