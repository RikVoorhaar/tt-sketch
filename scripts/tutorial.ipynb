{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage\n",
    "\n",
    "In this tutorial we guide you through the basic usage of this package. This tutorial is also available as a Jupyter notebook, and can for example be [run on google Colab by clicking this link](https://colab.research.google.com/github/RikVoorhaar/tt-sketch/blob/main/scripts/tutorial.ipynb).\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "This library can be installed using `pip` by running `pip install tt-sketch`. Alternatively in a Jupyter notebook or on Colab we can run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!pip install --quiet tt-sketch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also install the library by cloning the repository as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "    git clone git@github.com:RikVoorhaar/tt-sketch.git\n",
    "    cd tt-sketch\n",
    "    pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor train decompositions\n",
    "\n",
    "The purpose of this library is to quickly and easily compute low-rank Tensor Train (TT) decompositions of a given tensor. This is for example useful if we want to compress tensorial data, or to convert from some other low-rank tensor format to a TT format because of some of its attractive properties.\n",
    "\n",
    "A TT has a shape $(n_1, n_2, \\ldots, n_d)$ and a rank $(r_1,\\ldots,r_{d-1})$ associated to it. To create a random TT of specified rank and shape we can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor train of shape (10, 5, 6, 8, 5) with rank (4, 6, 3, 2) at 0x7f2af04b10d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tt_sketch.tensor import TensorTrain\n",
    "\n",
    "shape = (10, 5, 6, 8, 5)\n",
    "rank = (4, 6, 3, 2)\n",
    "tt = TensorTrain.random(shape, rank)\n",
    "tt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a TT into a numpy array as shown below. Note that this is usually not something you want to do,\n",
    "since a TT uses far less memory. Below we print the number of floating point numbers required for storing both\n",
    "the TT and the equivalent numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 6, 8, 5)\n",
      "12000\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "tt_numpy = tt.to_numpy()\n",
    "print(tt_numpy.shape)\n",
    "print(tt_numpy.size)\n",
    "print(tt.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TT is essentially a list of _cores_ which are order-3 tensors of shape $(r_{\\mu-1},n_\\mu,r_\\mu)$. We can access them using the `.cores` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 10, 4), (4, 5, 6), (6, 6, 3), (3, 8, 2), (2, 5, 1)]\n"
     ]
    }
   ],
   "source": [
    "cores = tt.cores\n",
    "print([C.shape for C in cores])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize a TT directly from a list of cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor train of shape (5, 6, 8) with rank (3, 4) at 0x7f2a20957760>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "C1 = np.random.normal(size=(1, 5, 3))\n",
    "C2 = np.random.normal(size=(3, 6, 4))\n",
    "C3 = np.random.normal(size=(4, 8, 1))\n",
    "TensorTrain([C1, C2, C3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is some basic arithmetic we can do with TTs, the main point of interest for us is to convert tensors to the TT format. Before we consider that problem we first introduce some other tensor formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor formats\n",
    "\n",
    "Other than TTs, the `tt_sketch.tensor` module has support for some other tensor formats. We list them here.\n",
    "\n",
    "### Sparse tensors\n",
    "\n",
    "A sparse tensor $\\mathcal T$ consists of a list of indices pointing to the location of non-zero entries, and a list of values of these non-zero entries. Below we construct a sparse tensor with 100 non-zero uniformly distributed elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sparse tensor of shape (10, 5, 6, 8) with 100 non-zero entries at 0x7f2a20957c10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tt_sketch.tensor import SparseTensor\n",
    "\n",
    "nnz = 100\n",
    "shape = (10, 5, 6, 8)\n",
    "\n",
    "# Generate `nnz` random indices and entries\n",
    "total_size = np.prod(shape)\n",
    "indices_flat = np.random.choice(total_size, size=nnz, replace=False)\n",
    "indices = np.unravel_index(indices_flat, shape)\n",
    "entries = np.random.uniform(size=nnz)\n",
    "\n",
    "sparse = SparseTensor(shape, indices, entries)\n",
    "\n",
    "# Alternative method:\n",
    "# sparse = SparseTensor.random(shape, nnz)\n",
    "\n",
    "sparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be converted to a dense tensor as before using `.to_numpy()`. Again, since this is a compressed tensor format, this can drastically increase the memory usage. Note that in this case we need to store 5 numbers for each non-zero entry of `sparse`; 4 for the index, and 1 for the value. This is akin to the COO format for sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 6, 8)\n",
      "2400\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "sparse_numpy = sparse.to_numpy()\n",
    "print(sparse_numpy.shape)\n",
    "print(sparse_numpy.size)\n",
    "print(sparse.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CP tensors\n",
    "A CP (a.k.a. CANDECOMP/PARAFAC) tensor is a sum of $N$ rank-1 tensors. Each of these rank one tensors is represented by a tuple of vectors: $v_1\\otimes v_2\\otimes \\cdots\\otimes v_d$. For each mode of the tensor, all these $N$ vectors are stored in an $n_\\mu\\times N$ matrix $V_\\mu$. Below we create a random rank 100 CP tensor. All matrices $V_\\mu$ are random normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CP tensor of shape (7, 5, 6, 20) and rank 100 at 0x7f2af062fd30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tt_sketch.tensor import CPTensor\n",
    "\n",
    "shape = (7, 5, 6, 20)\n",
    "rank = 100\n",
    "\n",
    "cores = [np.random.normal(size=(n, rank)) for n in shape]\n",
    "cp = CPTensor(cores)\n",
    "\n",
    "# Alternative method:\n",
    "# cp = CPTensor.random(shape, rank)\n",
    "\n",
    "cp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tucker tensors\n",
    "\n",
    "A Tucker tensor consists of a dense core tensor $\\mathcal C$ of shape $s_1\\times\\cdots\\times s_d$, and a collection of factor matrices $U_\\mu$ of shape $s_\\mu\\times n_\\mu$. This forms a $n_1\\times\\cdots\\times n_d$ tensor through the product $\\mathcal T = (U_1\\otimes\\cdots\\otimes U_d)\\mathcal C$. Often the matrices $U_\\mu$ are assumed to have orthogonal rows (so that they represent an orthogonal basis of a subspace), but this is not necessary for our purposes. We can create a Tucker tensors as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tucker tensor of shape (7, 4, 9, 4, 12) and rank (3, 4, 2, 2, 3) at 0x7f2af043a070>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tt_sketch.tensor import TuckerTensor\n",
    "\n",
    "shape = (7, 4, 9, 4, 12)\n",
    "rank = (3, 4, 2, 2, 3)\n",
    "\n",
    "factor_matrices = [np.random.normal(size=(r, n)) for r, n in zip(rank, shape)]\n",
    "core = np.random.normal(size=shape)\n",
    "tucker = TuckerTensor(factor_matrices, core)\n",
    "\n",
    "# Alternative method:\n",
    "# tucker = TuckerTensor.random(shape, rank)\n",
    "\n",
    "tucker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor sums\n",
    "\n",
    "One of the major advantages of our TT sketching algorithms is that they work very well for sums of arbitrary tensors (with the same shape). Taking sums of tensors is easy; the `+` operator is overloaded to create a `tt_sketch.tensor.TensorSum` object when taking the sum of any tensors. This is simply a container with a list of tensors. For example, let's add together a sparse tensor, a CP, and a TT together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sum of 3 tensors of shape (8, 8, 8, 4, 5) at 0x7f2af076ea90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (8, 8, 8, 4, 5)\n",
    "cp = CPTensor.random(shape, 100)\n",
    "sparse = SparseTensor.random(shape, 1000)\n",
    "tt = TensorTrain.random(shape, 10)\n",
    "\n",
    "tensor_sum = cp + sparse + tt\n",
    "tensor_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication by scalars is also supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sum of 3 tensors of shape (8, 8, 8, 4, 5) at 0x7f2a20957b80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp * 0.1 - tt + sparse * 1e-9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMT sketch\n",
    "\n",
    "This library implements three similar sketching algorithms for finding approximate TT decompositions; `orthogonal_sketch`, `stream_sketch` and `hmt_sketch`. The former method is unpublished because it offers little advantage over the other two methods, and therefore we will not cover it in this tutorial either. Perhaps the easiest to use  of the methods is the TT-HMT sketching procedure `tt_sketch.sketch.hmt_sketch`. In its most basic usage, we just need to supply a tensor, and a desired approximation rank. As a demonstration, we will approximate the `tensor_sum` defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor train of shape (8, 8, 8, 4, 5) with rank (2, 3, 4, 5) at 0x7f2a20957310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tt_sketch.sketch import hmt_sketch\n",
    "\n",
    "rank = (2, 3, 4, 5)\n",
    "tt_sketched = hmt_sketch(tensor_sum, rank)\n",
    "tt_sketched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however that this is not a very good approximation, because the tensor we are trying to approximate cannot be accurately represented by a TT of this low rank. We can check the quality of the approximation by computing the relative error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9900753801070363"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_sketched.error(tensor_sum, relative=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative error is quite close to 1, but for a good approximation it should be close to zero. \n",
    "\n",
    "As a comparison, we have also implemented the (much more expensive) classical TT-SVD algorithm, and it also fails to find a useful approximation of this tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9754118198082177"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tt_sketch.tt_svd import tt_svd\n",
    "\n",
    "tt_approx = tt_svd(tensor_sum, rank)\n",
    "tt_approx.error(tensor_sum, relative=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A tensor that is very easy to approximate with a TT is, of course, a TT. Reconstructing a TT using the sketching algorithm produces very accurate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.358868417360322e-14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (4, 5, 6, 8, 5)\n",
    "rank = (4, 6, 3, 2)\n",
    "tt = TensorTrain.random(shape, rank)\n",
    "\n",
    "tt_sketched = hmt_sketch(tt, rank)\n",
    "\n",
    "tt_sketched.error(tt, relative=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually not necessary to supply `rank` as a tuple. If we provide a single integer then a constant rank is assumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor train of shape (4, 5, 6, 8, 5) with rank (4, 8, 8, 5) at 0x7f2af51f0100>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmt_sketch(tt, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case the rank of the approximate TT is not `(8, 8, 8, 8)`. This is because the first and last dimensions of the tensor are less than the rank 8, and increasing the rank past the dimension will not improve the quality any further, and the rank is therefore automatically truncated. The maximum value for the second `left_rank[1]` would in this case be `4*5=20`, which is more than `8`, so no further truncation happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing DRM types\n",
    "\n",
    "The sketching is done by contracting the tensor with a Dimension Reduction Matrix (DRM). By default we use DRMs obtained through partial contractions of a TT (i.e. the class `TensorTrainDRM`), but other options are available as well. In particular for sparse tensors it makes sense to use a Gaussian DRM. \n",
    "\n",
    "For example, below we sketch a sparse tensor using a Gaussian DRM (`tt_sketched2`). Typically choosing a different DRM will not result in drastically different performance, although it may reduce the variance in the quality of the approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.166825938858844e-05\n",
      "2.0489501226846625e-05\n"
     ]
    }
   ],
   "source": [
    "from tt_sketch.drm import SparseGaussianDRM\n",
    "\n",
    "nnz = 100\n",
    "shape = (10, 10, 10, 10)\n",
    "sparse_tensor = SparseTensor.random(shape, nnz)\n",
    "sparse_tensor.entries *= np.logspace(0, -50, nnz)  # make entries decay fast\n",
    "\n",
    "tt_sketched1 = hmt_sketch(sparse_tensor, rank=10)\n",
    "print(tt_sketched1.error(sparse_tensor, relative=True))\n",
    "\n",
    "tt_sketched2 = hmt_sketch(\n",
    "    sparse_tensor,\n",
    "    rank=10,\n",
    "    drm_type=SparseGaussianDRM,\n",
    ")\n",
    "print(tt_sketched2.error(sparse_tensor, relative=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming sketch\n",
    "\n",
    "The second sketching algorithm is the Streaming TT approximation. While it typically produces errors that are\n",
    "slightly worse than the TT-HMT method, it has the advantage of being a streaming algorithm. This means\n",
    "that we can cheaply update the sketch of a tensor. If we don't care about this feature, the usage is very similar to `hmt_sketch`,\n",
    "except for one major difference. Since a two-sided sketch is performed, we need to supply both a `left_rank` and a `right_rank`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sketched tensor train of shape (10, 10, 10, 10) with left-rank (10, 10, 10) and right-rank (15, 15, 15) at 0x7f2af0635a90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tt_sketch.sketch import stream_sketch\n",
    "\n",
    "tt_sketched = stream_sketch(sparse_tensor, left_rank=10, right_rank=15)\n",
    "tt_sketched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the sketch is _not_ a `TensorTrain` object, but rather a `SketchedTensorTrain` object. It can easily be converted to a `TensorTrain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tensor train of shape (10, 10, 10, 10) with rank (10, 10, 10) at 0x7f2a203274f0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.523995632718617e-05"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = tt_sketched.to_tt()\n",
    "print(tt)\n",
    "tt.error(sparse_tensor, relative=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating a `SketchedTensorTrain` is easy; we simply add a new tensor to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.327928793048581e-05\n",
      "9.479464055761044e-05\n"
     ]
    }
   ],
   "source": [
    "other_sparse_tensor = SparseTensor.random(shape, 10)*1e-6\n",
    "sparse_tensor_sum = sparse_tensor + other_sparse_tensor\n",
    "\n",
    "# Updating an existing sketch\n",
    "tt_sketched_updated = tt_sketched + other_sparse_tensor\n",
    "print(tt_sketched_updated.error(sparse_tensor_sum, relative=True))\n",
    "\n",
    "# Sketching the sum of two tensors directly\n",
    "tt_sketched2 = stream_sketch(sparse_tensor_sum, left_rank=10, right_rank=15)\n",
    "print(tt_sketched2.error(sparse_tensor_sum, relative=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computes a sketch of `other_sparse_tensor` using the same DRMs and ranks. The resulting sketch is completely equivalent to sketching the sum of tensors directly. \n",
    "\n",
    "An important restriction is that all entries of `left_rank` need to be smaller than all entries of `right_rank` (or vice versa).\n",
    "Just as before we can also use other DRMs. If we use `SparseGaussianDRM`, we can also adaptively increase the rank of our approximation. This is useful if we are not sure in advance what tt-rank we should choose. For technical reasons this is not possible for `TensorTrainDRM`.\n",
    "\n",
    " For example below we first try to approximate `sparse_tensor` as a rank 5 TT, but then realize this is not good enough, and we increase the rank to `(10, 15, 10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Sketched tensor train of shape (10, 10, 10, 10) with left-rank (10, 10, 10) and right-rank (5, 5, 5) at 0x7f2a2036bfd0>\n",
      "relative error: 0.011087215612540783\n",
      "\n",
      "<Sketched tensor train of shape (10, 10, 10, 10) with left-rank (20, 30, 20) and right-rank (10, 15, 10) at 0x7f2af0635910>\n",
      "relative error: 1.376905732926918e-07\n"
     ]
    }
   ],
   "source": [
    "tt_sketched = stream_sketch(\n",
    "    sparse_tensor,\n",
    "    left_rank=10,\n",
    "    right_rank=5,\n",
    "    left_drm_type=SparseGaussianDRM,\n",
    "    right_drm_type=SparseGaussianDRM,\n",
    ")\n",
    "\n",
    "print(tt_sketched)\n",
    "print(\"relative error:\", tt_sketched.error(sparse_tensor, relative=True))\n",
    "\n",
    "tt_sketched_updated = tt_sketched.increase_rank(\n",
    "    sparse_tensor, (20, 30, 20), (10, 15, 10)\n",
    ")\n",
    "print(\"\")\n",
    "print(tt_sketched_updated)\n",
    "print(\n",
    "    \"relative error:\", tt_sketched_updated.error(sparse_tensor, relative=True)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e3584a94e8c7d99898d4a2cb47bea56236d638682d6d87741bc73b63e7db14"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tensors')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
